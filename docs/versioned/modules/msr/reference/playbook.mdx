---
sidebar_position: 3
sidebar_label: Playbook
---

# MSR Operational Playbook

This playbook provides practical guidance for backup, recovery, and disaster response procedures for Multi-Session Replay (MSR) deployments.

## Overview

MSR's disaster recovery strategy leverages **Kafka's durability as the primary source of truth** for CDC events, combined with periodic database snapshots to enable zero-data-loss recovery.

### Core Principle: Baseline + Deltas

CDC events in Kafka are **change deltas**, not absolute state. Recovery requires:

1. **Baseline State** - Full database dump taken at time T
2. **Delta Changes** - Kafka CDC events from time T to present
3. **Recovery** - Restore baseline + replay deltas

:::info Why You Need Both
If an entity was created on Day 50 and Kafka only retains 7 days of events (Day 173-180), you need the Day 173 database dump to know the entity existed. You cannot replay from Kafka alone.
:::

## Backup Strategy

### Choosing Your Backup Approach

Select based on your infrastructure:

| Infrastructure | Recommended Approach | Complexity | Cost |
|----------------|---------------------|------------|------|
| **Timescale Cloud** | Native Cloud Backups | Low | Included |
| **AWS EC2 + EBS** | EBS Volume Snapshots | Low | Low (incremental) |
| **Kubernetes + PVC** | Volume Snapshots (Velero/CSI) | Medium | Low-Medium |
| **Self-Managed** | pg_dump + Kafka | Medium | Low |
| **Hybrid** (Production) | Volume snapshots + pg_dump | Medium | Low |

:::warning TimescaleDB Compatibility
MSR requires TimescaleDB. Standard cloud PostgreSQL services (AWS RDS/Aurora, Azure Database, GCP Cloud SQL) are **not compatible** as they don't support TimescaleDB extensions.
:::

### Daily Backup Workflow

**Minimum viable backup:**
```bash
# Full database dump (contains everything)
pg_dump -h localhost -U postgres -d msr_db -n msr -Fc -f msr_backup.dump
```

**Production-ready backup** (with metadata):
```bash
#!/bin/bash
# 1. Full database dump
pg_dump -h localhost -U postgres -d msr_db -n msr -Fc \
    -f /backups/msr_full_$(date +%Y%m%d_%H%M%S).dump

# 2. Configuration (quick access)
psql -h localhost -U postgres -d msr_db \
    -c "COPY msr.configuration TO STDOUT WITH CSV HEADER" \
    > /backups/msr_config_$(date +%Y%m%d_%H%M%S).csv

# 3. Capture Kafka offset at backup time
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --describe --group msr-sink-connector \
    --offsets > /backups/kafka_offsets_$(date +%Y%m%d_%H%M%S).txt
```

:::tip Backup Timing
Schedule backups AFTER MSR maintenance completes. If maintenance runs at 3:00 AM, schedule backups at 3:30 AM.
:::

### Kafka Retention Configuration

**Decision Matrix:**

| Backup Frequency | Recommended Kafka Retention | Buffer |
|------------------|----------------------------|--------|
| Daily | 3-7 days | Covers weekends |
| Twice Daily | 2-3 days | Shorter window |
| Hourly | 1-2 days | Minimal window |

**Configuration:**
```yaml
# Kafka/Redpanda topic configuration
retention.ms: 604800000  # 7 days in milliseconds
compression.type: lz4
cleanup.policy: delete
```

## Recovery Procedures

### Procedure 1: Standard Recovery (Zero Data Loss)

**When to use:** Complete database loss, corruption

**Prerequisites:**
- Database backup within Kafka retention window
- Kafka cluster is healthy
- Kafka retains events from backup time

**Steps:**

```bash
#!/bin/bash
# 1. Restore database from backup
pg_restore -h localhost -U postgres -d msr_db \
    --clean --if-exists \
    /backups/msr_full_20251027.dump

# 2. Calculate Kafka offset at backup time
BACKUP_TIME="2025-10-27T03:00:00Z"
BACKUP_MS=$(date -d "${BACKUP_TIME}" +%s)000

KAFKA_OFFSET=$(kafka-run-class.sh kafka.tools.GetOffsetShell \
    --broker-list kafka:9092 \
    --topic gis.source.topic \
    --time "${BACKUP_MS}" | cut -d':' -f3)

# 3. Reset Kafka consumer group to backup offset
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --group msr-sink-connector \
    --topic gis.source.topic \
    --reset-offsets --to-offset "${KAFKA_OFFSET}" \
    --execute

# 4. Restart Kafka Connect sink connector
curl -X DELETE "http://kafka-connect:8083/connectors/msr-sink-connector"
curl -X POST "http://kafka-connect:8083/connectors" \
    -H "Content-Type: application/json" \
    -d @/config/msr-sink-connector.json

# 5. Monitor replay progress
while true; do
    LAG=$(kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
        --describe --group msr-sink-connector | \
        awk 'NR>1 {sum+=$6} END {print sum}')

    echo "Current Kafka lag: ${LAG}"

    if [ "${LAG}" -lt 100 ]; then
        echo "Replay complete!"
        break
    fi

    sleep 30
done

# 6. Rebuild derived structures
psql -h localhost -U postgres -d msr_db <<EOF
CALL refresh_continuous_aggregate('msr.entity_last_states', NULL, NULL);
SELECT msr.refresh_earliest_snapshot();
EOF
```

**Expected RTO:** 1-4 hours (depends on volume of CDC events to replay)
**Expected RPO:** Zero data loss (within Kafka retention)

### Procedure 2: Point-in-Time Recovery

**When to use:** Recover to specific timestamp (e.g., before data corruption)

**Steps:**

```bash
#!/bin/bash
TARGET_TIME="2025-10-27T10:00:00Z"  # Time before corruption
BACKUP_FILE="/backups/msr_full_20251027_030000.dump"

# 1. Restore backup
pg_restore -h localhost -U postgres -d msr_db \
    --clean --if-exists "${BACKUP_FILE}"

# 2. Calculate Kafka offset for target time
TARGET_MS=$(date -d "${TARGET_TIME}" +%s)000
TARGET_OFFSET=$(kafka-run-class.sh kafka.tools.GetOffsetShell \
    --broker-list kafka:9092 \
    --topic gis.source.topic \
    --time "${TARGET_MS}" | cut -d':' -f3)

# 3. Reset consumer group to target offset
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --group msr-sink-connector \
    --topic gis.source.topic \
    --reset-offsets --to-offset "${TARGET_OFFSET}" \
    --execute

# 4. Restart connector (will replay up to target time)
# ... (same as Procedure 1)
```

**Expected RTO:** 1-2 hours
**Expected RPO:** Precise to the second

### Procedure 3: Cold Backup Recovery (Fallback)

**When to use:** Both database AND Kafka data lost (disaster scenario)

**Steps:**

```bash
#!/bin/bash
# Restore most recent backup only
pg_restore -h localhost -U postgres -d msr_db \
    --clean --if-exists \
    /backups/msr_full_latest.dump

# Verify restoration
psql -h localhost -U postgres -d msr_db <<EOF
SELECT COUNT(*) as total_events FROM msr.cdc_event;
SELECT current_snapshot FROM msr.snapshot_pointer;
EOF
```

:::danger Data Loss
This method accepts data loss between backup time and failure time. Only use when Kafka events are unavailable.
:::

**Expected RTO:** 15-30 minutes
**Expected RPO:** Last backup frequency (e.g., 24 hours for daily backups)

## Daily Operations

### Health Checks

Run these queries daily to ensure system health:

```sql
-- Check 1: Verify recent backup exists
SELECT
    CASE
        WHEN age(now(), MAX(created_at)) < INTERVAL '25 hours'
        THEN 'OK'
        ELSE 'ALERT: No backup in 24 hours'
    END as backup_status
FROM msr.configuration;

-- Check 2: Verify snapshot health
SELECT
    current_snapshot,
    last_refresh,
    cutoff_time,
    CASE
        WHEN age(now(), last_refresh) < INTERVAL '25 hours'
        THEN 'OK'
        ELSE 'ALERT: Snapshot not refreshed'
    END as snapshot_status
FROM msr.snapshot_pointer;

-- Check 3: Monitor CDC event ingestion
SELECT
    MAX(event_timestamp) as latest_event,
    age(now(), MAX(event_timestamp)) as lag,
    COUNT(*) as total_events
FROM msr.cdc_event;
```

**Kafka lag monitoring:**
```bash
# Check consumer lag (should be < 10,000)
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --describe --group msr-sink-connector
```

### Automated Monitoring Script

```bash
#!/bin/bash
# File: /scripts/health_check.sh

ALERT_EMAIL="ops-team@company.com"

# Check backup age
BACKUP_AGE=$(find /backups/msr -name "msr_*.dump" -mtime -1 | wc -l)
if [ "${BACKUP_AGE}" -eq 0 ]; then
    echo "ALERT: No backup in last 24 hours" | \
        mail -s "MSR Backup Alert" "${ALERT_EMAIL}"
fi

# Check Kafka lag
HIGH_LAG=$(kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --describe --group msr-sink-connector | \
    awk '$6 > 10000 {print $0}')

if [ -n "${HIGH_LAG}" ]; then
    echo "ALERT: High Kafka lag detected\n${HIGH_LAG}" | \
        mail -s "MSR Kafka Lag Alert" "${ALERT_EMAIL}"
fi
```

**Crontab:**
```cron
# Run health checks every 4 hours
0 */4 * * * /scripts/health_check.sh >> /var/log/msr_health.log 2>&1
```

## Disaster Response Matrix

| Scenario | Primary Response | Fallback | Expected RTO |
|----------|------------------|----------|--------------|
| MSR DB corruption (partial) | Point-in-Time Recovery | Cold Backup | 1-2 hours |
| MSR DB complete loss | Standard Recovery | Cold Backup | 1-4 hours |
| Configuration corruption | Config-only restore | Point-in-Time | 5 minutes |
| Kafka data loss | Cold Backup Recovery | Manual reconciliation | 30 minutes |
| Both DB + Kafka loss | Cold Backup Recovery | Accept data loss | 30 minutes |

### Quick Decision Tree

```
Database failure detected
    ↓
Is Kafka healthy?
    ├─ YES → Use Standard Recovery (Procedure 1)
    │         • Zero data loss
    │         • RTO: 1-4 hours
    │
    └─ NO → Use Cold Backup Recovery (Procedure 3)
              • Accept data loss
              • RTO: 30 minutes
```

## Recovery Testing Schedule

### Monthly Testing (5 minutes)
```bash
# Test configuration restore
cp /backups/latest/msr_config.csv /tmp/
psql -h test-db -U postgres -d test_msr <<EOF
TRUNCATE TABLE msr.configuration;
COPY msr.configuration FROM '/tmp/msr_config.csv' WITH CSV HEADER;
EOF
```

### Quarterly Testing (2 hours)
```bash
# Full recovery simulation in staging environment
# 1. Create test database
# 2. Run Standard Recovery procedure
# 3. Validate data integrity
# 4. Measure actual RTO
# 5. Document findings
```

### Annual Testing (4 hours)
- Complete disaster simulation
- Test all recovery procedures
- Verify all scripts and documentation
- Update procedures based on lessons learned

## Validation After Recovery

Run this checklist after any recovery:

```bash
#!/bin/bash
# File: /scripts/validate_recovery.sh

echo "=== MSR Recovery Validation ==="

# Test 1: Schema integrity
TABLES=$(psql -h localhost -U postgres -d msr_db -t -c \
    "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='msr';")
if [ "${TABLES}" -ne 7 ]; then
    echo "❌ FAIL: Expected 7 tables, found ${TABLES}"
    exit 1
fi
echo "✅ PASS: All tables present"

# Test 2: Hypertable status
HYPERTABLE=$(psql -h localhost -U postgres -d msr_db -t -c \
    "SELECT hypertable_name FROM timescaledb_information.hypertables
     WHERE hypertable_name='cdc_event';")
if [ -z "${HYPERTABLE}" ]; then
    echo "❌ FAIL: Hypertable not found"
    exit 1
fi
echo "✅ PASS: Hypertable configured"

# Test 3: Continuous aggregate
CAGG=$(psql -h localhost -U postgres -d msr_db -t -c \
    "SELECT view_name FROM timescaledb_information.continuous_aggregates
     WHERE view_name='entity_last_states';")
if [ -z "${CAGG}" ]; then
    echo "❌ FAIL: Continuous aggregate not found"
    exit 1
fi
echo "✅ PASS: Continuous aggregate present"

# Test 4: Data sanity
psql -h localhost -U postgres -d msr_db <<EOF
SELECT
    'CDC Events' as metric,
    COUNT(*)::text as count,
    MIN(event_timestamp)::text as oldest,
    MAX(event_timestamp)::text as newest
FROM msr.cdc_event;
EOF

echo "✅ All validation tests passed"
```

## Configuration Alignment

### Kafka Retention vs. Backup Frequency

:::danger Critical Configuration
`MAX_PLAYBACK_RANGE` must be ≤ Kafka retention period. If Kafka retains 7 days, `MAX_PLAYBACK_RANGE` must be 7 days or less.
:::

```sql
-- Check current configuration
SELECT config_key, value
FROM msr.configuration
WHERE config_key IN (
    'MAX_PLAYBACK_RANGE',
    'DATA_RETENTION_CRON_EXPRESSION',
    'MAX_ACTIVE_SESSIONS'
);

-- Align with Kafka retention (7 days)
UPDATE msr.configuration
SET value = '7'
WHERE config_key = 'MAX_PLAYBACK_RANGE';
```

### Backup Schedule Alignment

```
MSR Maintenance:    3:00 AM ──→ refresh_earliest_snapshot()
                                 └─ Rebuilds snapshots
                                 └─ Cleans old CDC events
                                    ↓
Backup Schedule:    3:30 AM ──→ pg_dump + metadata capture
                                 └─ Captures consistent state
                                 └─ Records Kafka offset
```

## Emergency Contacts

### Escalation Matrix

| Severity | Scenarios | Response Time | Escalate To |
|----------|-----------|---------------|-------------|
| **Critical** | Database complete loss<br/>Kafka cluster failure<br/>Both DB + Kafka failure | 15 minutes | CTO |
| **High** | Data corruption<br/>Backup failures (>24h)<br/>High Kafka lag (>50,000) | 1 hour | Engineering Manager |
| **Medium** | Snapshot maintenance failure<br/>Configuration drift | 4 hours | Team Lead |

### On-Call Rotation
- **Primary**: ops-team@company.com
- **Secondary**: platform-team@company.com
- **Escalation**: engineering-manager@company.com

## Quick Reference Commands

### Backup Operations
```bash
# Create backup
/scripts/backup_msr.sh                    # 5-15 min

# List recent backups
ls -lth /backups/msr/*.dump | head -5

# Verify backup integrity
tar -tzf /backups/msr/latest/msr_backup.tar.gz > /dev/null
```

### Recovery Operations
```bash
# Standard recovery (zero data loss)
/scripts/recover_standard.sh             # 1-4 hours

# Point-in-time recovery
/scripts/recover_point_in_time.sh \
    "2025-10-27T10:00:00Z" \
    /backups/msr_full_20251027.dump      # 1-2 hours

# Cold backup (fallback)
/scripts/recover_cold_backup.sh \
    /backups/msr_full_latest.dump        # 30 min
```

### Monitoring
```bash
# Health check
/scripts/health_check.sh                 # 1 min

# Validate recovery
/scripts/validate_recovery.sh            # 2 min

# Check Kafka lag
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --describe --group msr-sink-connector
```

### Kafka Operations
```bash
# List consumer groups
kafka-consumer-groups.sh --bootstrap-server kafka:9092 --list

# Reset to specific timestamp
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --group msr-sink-connector \
    --topic gis.source.topic \
    --reset-offsets --to-datetime 2025-10-27T10:00:00.000 \
    --execute

# Reset to earliest
kafka-consumer-groups.sh --bootstrap-server kafka:9092 \
    --group msr-sink-connector \
    --topic gis.source.topic \
    --reset-offsets --to-earliest \
    --execute
```

## Additional Resources

- **Full DR Strategy**: See `docs/DISASTER_RECOVERY_STRATEGY.md` in MSR repository
- **TimescaleDB Backup Guide**: https://docs.timescale.com/self-hosted/latest/backup-and-restore/
- **Kafka Documentation**: https://kafka.apache.org/documentation/
- **Debezium CDC**: https://debezium.io/documentation/

---

**Last Updated**: 2025-10-27
**Version**: 1.0
